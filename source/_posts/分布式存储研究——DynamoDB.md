---
title: 分布式存储研究——DynamoDB
date: 2019-08-01 20:39:59
category: 论文研读
tags:
- 分布式
- 数据库
- Amazon
- DynamoDB
---

这篇博客，分享另一个云端巨头：Amazon的DynamoDB数据库的实现论文，“Dynamo: Amazon's Highly Available Key-value Store”。这也是整个专题的第三篇。

<!-- more -->

# Background

## System Assumptions and Requirements

与GFS，Bigtable等一样，DynamoDB有一些适用的场景。文章做出以下假设，并根据这些假设进行系统的设计，会更有针对性：

+ Query Model：读写操作简单，且均基于主键。没有跨多行的操作，也不需要关系语义。存储对象通常在1MB以下。
+ ACID Properties：Dynamo提供的一致性较弱，不保障隔离性（isolation），且只执行单主键（单行）的更新。
+ Efficiency：系统需要满足高效率，需要在性能、成本效益、可用性和Durability保障之间进行权衡。
+ 数据库用于内部系统，不会被针对性攻击，不需要进行鉴权授权。
+ 数据库的目标是扩展到上百个数据节点。

## Service Level Agreements (SLA)

在做数据库的性能评估时，亚马逊更倾向于使用99.9%的分布值来进行衡量，代替常用的平均值、中位数、方差期望等等。这和我们在做一些性能测试时很相像，TP999更能反应系统的一般情况，覆盖近乎所有人，而非大部分人的体验。

## Design Considerations

CAP理论说明，在区间通信失败或一些网络失败时，数据一致性和高可用性无法同时保证。DynamoDB期望保障数据的最终一致性，牺牲了一定的一致性等级来维护服务高可用。此时就需要对一些修改冲突进行处理。很多传统数据库通常在写时进行冲突处理，保证读的逻辑简洁，这会导致一些无法写到所有分片的写操作失败。而DynamoDB希望能有一个始终可写的数据存储，不希望驳回用户的写请求。那么此时就会把一些冲突处理迁移到读的过程中去。

另一个设计理念，是确立由谁来解决写冲突。DynamoDB设计为可以由服务调用方来使用不同的规则处理修改冲突，也可以将这一职责下放到数据库，由数据库执行一些简单的策略，如“last write wins”。

Dynamo还应该被设计为可以持续扩展，即在原有集群上能水平扩展一个节点，同时不对原系统产生太多的影响。

Dynamo的所有节点职责应该是一致的，也就是没有“主节点”的概念，简化系统的运维工作。这称为“Symmetry”

比Symmentry更进一步，是Decentralization，去中心化，使用点对点而非中心控制的模式。

最后介绍的一个设计理念是“Heterogeneity”，异质性，系统应当有利用基础硬件异质性的能力，如在机器间根据其实际能力进行均衡负载。

> 异质性这部分更多的是我个人的理解，还需要在后续的阅读中明确是否一致。

# Related Work

第一代P2P系统，称为非结构化P2P网络，如Freenet，Gnutella，常用于文件分享，会做分布式存储，但每个请求会尽可能请求所有节点来获取数据。第二代P2P系统，称为结构化网络，可以进行合理正确地路由，快速访问有所需要数据的节点。代表如Pastry和Chord。在此基础上，搭建了不少数据存储系统，如Oceanstore和PAST。

除了P2P系统，也有很多分布式文件存储系统，例如Bayou，Coda，Ficus，Farsite system以及之前研读过的GFS，Bigtable，还有FAB，Antiquity等等。

与所有列举的系统不一样地方：

+ Dynamo强调始终可写；
+ 系统被预期部署在一个所有节点可信任的环境中；
+ 使用Dynamo的应用不需要支持结构性的命名空间或是复杂的关系语义，只需要简单的key-value；
+ Dynamo针对时延敏感应用，需要保证TP999在几百毫秒的级别。

# System Architecture

Dynamo针对关键功能所采用的策略见下表：

![Dynamo技术](https://hotlinkqiu-blog.oss-cn-shenzhen.aliyuncs.com/dynamo/dynamo_techniques.PNG)

## System Interface

系统提供简单的put和get接口来操作数据。get可以获取到一个数据，也可以是一组冲突的数据，交由应用来解决冲突。put则需要加上context信息，包含了一些版本信息等。系统通过对key进行128位的MD5 hash来决定存储数据的服务器。

## Partitioning Algorithm

分区算法要求能够动态地将数据平衡分布到各个节点。Dynamo主要通过对key进行hash，值域得到一个环，然后分布到各个节点中去。每个数据会放在hash得到对应的节点，以及其后一个节点上。

这是最基本的哈希一致性算法，其问题是随机分配会导致数据和位置在环上不均匀，同时也忽略了节点性能的异质性。为此Dynamo做了hash算法的变种，每个实体节点会分配多个环中的虚拟节点。这样有几点好处：

+ 如果一个真实节点不可用，其负载会被剩余节点平分（想象每个数据有两个副本，真实节点1可能存储的是虚拟1、4、7节点，挂掉时会访问对应的2、5、8节点分布在三个不同的真实节点上）。
+ 当一个节点恢复可用，或加入到系统中时，各个其他可用节点可以分担数据迁移的压力。
+ 一个节点具体负责的虚拟节点数量可以根据其容量，性能动态调整，可以感受到硬件节点的异质性。